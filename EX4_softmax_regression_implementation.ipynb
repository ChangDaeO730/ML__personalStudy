{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> sklearn없이 numpy만 사용하여  \n",
    "미니배치경사하강법 + 조기종료 소프트맥스회귀 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris  = datasets.load_iris()\n",
    "X, y = iris['data'], iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(X, y, test_ratio = 0.2, seed = 42):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    data_len = X.shape[0]\n",
    "    ran_idx = np.random.permutation(data_len)\n",
    "    \n",
    "    X, y = X[ran_idx], y[ran_idx]\n",
    "    X_train, X_test, y_train, y_test = (\n",
    "        X[:int(data_len * (1-test_ratio))], X[int(data_len * (1-test_ratio)):],\n",
    "        y[:int(data_len * (1-test_ratio))], y[int(data_len * (1-test_ratio)):]\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = data_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid = X_train_full[:-20], X_train_full[-20:]\n",
    "y_train, y_valid = y_train_full[:-20], y_train_full[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sub component : one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = len(np.unique(y_train))   \n",
    "data_len = len(X_train)\n",
    "\n",
    "onehot_matrix = np.zeros((n_class, data_len))\n",
    "onehot_matrix[y_train, np.arange(data_len)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sub component : softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# np.apply_along_axis(softmax, 1, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sub component : initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight initalization (Xavier)\n",
    "fan_in = n_feature\n",
    "fan_out = n_class\n",
    "\n",
    "sigma_W = 2 / (fan_in + fan_out)\n",
    "W = sigma_W * np.random.randn(n_class, n_feature + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sub component : forward pass calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = len(X_train)\n",
    "\n",
    "X_train_with_bias = np.concatenate((np.ones((data_len, 1)), X_train), axis = 1)\n",
    "\n",
    "class_score_matrix = np.dot(W, X_train_with_bias.T)\n",
    "class_probability = np.apply_along_axis(softmax, 0, class_score_matrix)\n",
    "prediction = class_probability.argmax(axis = 0)\n",
    "\n",
    "loss = - (onehot_matrix * np.log(class_probability)).sum() / data_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = len(X_train)\n",
    "\n",
    "eta = 0.01 \n",
    "n_iter = 5001\n",
    "eps = 1e-7\n",
    "\n",
    "min_loss = float(\"inf\")\n",
    "patience = 0\n",
    "terminate = False\n",
    "\n",
    "history = {}\n",
    "epochs = []\n",
    "losses = []\n",
    "weights = []\n",
    "\n",
    "def fit(X_train, y_train, X_valid, y_valid, n_iter, patience_upper = 20):\n",
    "    for iteration in range(n_iter):\n",
    "        # train loss\n",
    "        class_score_matrix = np.dot(W, X_train_with_bias.T)\n",
    "        class_probability = np.apply_along_axis(softmax, 0, class_score_matrix)\n",
    "        train_loss = - (onehot_matrix * np.log(class_probability)).sum() / data_len\n",
    "\n",
    "        # parameters update\n",
    "        gradient = np.dot((class_probability - onehot_matrix), X_train_with_bias) / data_len\n",
    "        W = W - eta * gradient\n",
    "\n",
    "        # validation loss\n",
    "        val_class_score_matrix = np.dot(W, X_valid_with_bias.T)\n",
    "        val_class_probability = np.apply_along_axis(softmax, 0, val_class_score_matrix)\n",
    "        val_loss = - (val_onehot_matrix * np.log(val_class_probability)).sum() / val_len\n",
    "\n",
    "        # monitoring\n",
    "        if iteration % 500 == 0:\n",
    "            print(\"iter{}, train_loss : {}, valid_loss : {}\", iteration, loss, val_loss)\n",
    "\n",
    "        if val_loss < min_loss:\n",
    "            min_loss = val_loss\n",
    "\n",
    "            # saving history\n",
    "            epochs.append(iteration)\n",
    "            losses.append(min_loss)\n",
    "            weights.append(W)\n",
    "\n",
    "        # stopping rule\n",
    "        else :\n",
    "            patience += 1\n",
    "            if patience == patience_upper: \n",
    "                terminate = True\n",
    "                print('** Early Stopped **') \n",
    "\n",
    "        # End flow\n",
    "        if terminate == True:\n",
    "            history['epochs'] = epochs\n",
    "            history['val_losses'] = losses\n",
    "            history['weights'] = weights\n",
    "            break\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regressor (순한맛)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def y_to_onehot(y, n_class):\n",
    "    data_len = len(y)\n",
    "\n",
    "    onehot_matrix = np.zeros((n_class, data_len))\n",
    "    onehot_matrix[y, np.arange(data_len)] = 1\n",
    "    return onehot_matrix\n",
    "\n",
    "def add_bias(x):\n",
    "    return np.concatenate((np.ones((len(x), 1)), x), axis = 1)\n",
    "\n",
    "\n",
    "class SoftmaxRegressor(object):\n",
    "    def __init__(self, W = None, eta = 0.01, max_iter = 5001, history = None):\n",
    "        self.W = W\n",
    "        self.eta = eta\n",
    "        self.max_iter = max_iter\n",
    "        self.eps = 1e-7\n",
    "        self.history = history\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_valid, y_valid, patience_upper = 20):\n",
    "        self.history = {}\n",
    "        terminate = False\n",
    "        epochs = []\n",
    "        losses = []\n",
    "        weights = []\n",
    "        \n",
    "        n_feature = X_train.shape[1]\n",
    "        n_class = len(np.unique(y_train))\n",
    "        train_data_len = len(X_train)\n",
    "        val_data_len = len(X_valid)\n",
    "        min_loss = float(\"inf\")\n",
    "\n",
    "        X_train_with_bias, X_valid_with_bias = add_bias(X_train), add_bias(X_valid)\n",
    "        onehot_matrix = y_to_onehot(y_train, n_class)\n",
    "        val_onehot_matrix = y_to_onehot(y_valid, n_class)\n",
    "\n",
    "        # weight initalization (Xavier)\n",
    "        fan_in = n_feature\n",
    "        fan_out = n_class\n",
    "\n",
    "        sigma_W = 2 / (fan_in + fan_out)\n",
    "        self.W = sigma_W * np.random.randn(n_class, n_feature + 1)\n",
    "\n",
    "\n",
    "        # learning algorithm\n",
    "        for iteration in range(self.max_iter):\n",
    "            # train loss\n",
    "            class_score_matrix = np.dot(self.W, X_train_with_bias.T)\n",
    "            class_probability = np.apply_along_axis(softmax, 0, class_score_matrix)\n",
    "            train_loss = - (onehot_matrix * np.log(class_probability)).sum() / train_data_len\n",
    "\n",
    "            # parameters update\n",
    "            gradient = np.dot((class_probability - onehot_matrix), X_train_with_bias) / train_data_len\n",
    "            self.W = self.W - self.eta * gradient\n",
    "\n",
    "            # validation loss\n",
    "            val_class_score_matrix = np.dot(self.W, X_valid_with_bias.T)\n",
    "            val_class_probability = np.apply_along_axis(softmax, 0, val_class_score_matrix)\n",
    "            val_loss = - (val_onehot_matrix * np.log(val_class_probability)).sum() / val_data_len\n",
    "\n",
    "            # monitoring\n",
    "            if iteration % 500 == 0:\n",
    "                print(\"iter : {}, train_loss : {}, valid_loss : {}\".format(iteration, \n",
    "                                                                           round(train_loss,4), \n",
    "                                                                           round(val_loss,4)))\n",
    "\n",
    "            if val_loss < min_loss:\n",
    "                min_loss = val_loss\n",
    "\n",
    "                # saving history\n",
    "                epochs.append(iteration)\n",
    "                losses.append(min_loss)\n",
    "                weights.append(self.W)\n",
    "\n",
    "            # stopping rule\n",
    "            else :\n",
    "                patience += 1\n",
    "                if patience == patience_upper: \n",
    "                    terminate = True\n",
    "                    print('** Early Stopped **') \n",
    "\n",
    "            # End flow\n",
    "            if iteration == self.max_iter - 1:\n",
    "                terminate = True\n",
    "                \n",
    "            if terminate == True:\n",
    "                self.history['epochs'] = epochs\n",
    "                self.history['val_losses'] = losses\n",
    "                self.history['weights'] = weights\n",
    "                break\n",
    "    \n",
    "        return self.history\n",
    "\n",
    "\n",
    "    def predict(self, X, return_prob = False):\n",
    "        X_with_bias = add_bias(X)\n",
    "            \n",
    "        final_W = self.history['weights'][-1]\n",
    "            \n",
    "        class_score_matrix = np.dot(final_W, X_with_bias.T)\n",
    "        class_probability = np.apply_along_axis(softmax, 0, class_score_matrix)\n",
    "            \n",
    "        if return_prob == True:\n",
    "            return class_probability\n",
    "            \n",
    "        prediction = class_probability.argmax(axis = 0)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter : 0, train_loss : 2.7599, valid_loss : 2.7676\n",
      "iter : 500, train_loss : 0.448, valid_loss : 0.4775\n",
      "iter : 1000, train_loss : 0.3593, valid_loss : 0.3865\n",
      "iter : 1500, train_loss : 0.3093, valid_loss : 0.3351\n",
      "iter : 2000, train_loss : 0.2748, valid_loss : 0.2998\n",
      "iter : 2500, train_loss : 0.2492, valid_loss : 0.2736\n",
      "iter : 3000, train_loss : 0.2295, valid_loss : 0.2533\n",
      "iter : 3500, train_loss : 0.2138, valid_loss : 0.237\n",
      "iter : 4000, train_loss : 0.201, valid_loss : 0.2236\n",
      "iter : 4500, train_loss : 0.1904, valid_loss : 0.2124\n",
      "iter : 5000, train_loss : 0.1814, valid_loss : 0.2028\n"
     ]
    }
   ],
   "source": [
    "sr = SoftmaxRegressor()\n",
    "\n",
    "history = sr.fit(X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 2 0 1 2]\n",
      "[1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 2 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(sr.predict(X_test))\n",
    "print(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
